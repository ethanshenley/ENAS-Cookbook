{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recipe 3: ENAS with Policy Gradient (PPO)\n",
    "\n",
    "This notebook implements ENAS using Proximal Policy Optimization (PPO) for the controller update.\n",
    "\n",
    "## Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import Network\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Define constants\n",
    "NUM_OPS = 5\n",
    "NUM_NODES = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PPO Controller Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPOController(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTMCell(NUM_OPS, hidden_size)\n",
    "        self.actor = nn.Linear(hidden_size, NUM_OPS)\n",
    "        self.critic = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, num_cells):\n",
    "        h, c = torch.zeros(1, self.lstm.hidden_size), torch.zeros(1, self.lstm.hidden_size)\n",
    "        actions = []\n",
    "        log_probs = []\n",
    "        values = []\n",
    "        for _ in range(num_cells):\n",
    "            cell_actions = []\n",
    "            for _ in range(NUM_NODES * 3):\n",
    "                x = torch.zeros(1, NUM_OPS)\n",
    "                h, c = self.lstm(x, (h, c))\n",
    "                logits = self.actor(h)\n",
    "                value = self.critic(h)\n",
    "                probs = torch.softmax(logits, dim=-1)\n",
    "                action = torch.multinomial(probs, 1).item()\n",
    "                cell_actions.append(action)\n",
    "                log_probs.append(torch.log(probs[0, action]))\n",
    "                values.append(value)\n",
    "            actions.append(cell_actions)\n",
    "        return actions, torch.stack(log_probs), torch.cat(values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PPO Update Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ppo_update(ppo_controller, old_log_probs, old_values, rewards, actions, epsilon=0.2):\n",
    "    new_actions, new_log_probs, new_values = ppo_controller(len(actions))\n",
    "    \n",
    "    ratios = torch.exp(new_log_probs - old_log_probs)\n",
    "    advantages = rewards - old_values.detach()\n",
    "    \n",
    "    surr1 = ratios * advantages\n",
    "    surr2 = torch.clamp(ratios, 1 - epsilon, 1 + epsilon) * advantages\n",
    "    \n",
    "    actor_loss = -torch.min(surr1, surr2).mean()\n",
    "    critic_loss = nn.MSELoss()(new_values, rewards)\n",
    "    \n",
    "    loss = actor_loss + 0.5 * critic_loss\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_enas_ppo(network, ppo_controller, train_data, val_data, num_epochs):\n",
    "    network_optim = optim.Adam(network.parameters(), lr=0.01)\n",
    "    ppo_optim = optim.Adam(ppo_controller.parameters(), lr=0.001)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Train shared parameters\n",
    "        network.train()\n",
    "        for x, y in train_data:\n",
    "            actions, _, _ = ppo_controller(len(network.cells))\n",
    "            network_optim.zero_grad()\n",
    "            loss = nn.CrossEntropyLoss()(network(x, actions), y)\n",
    "            loss.backward()\n",
    "            network_optim.step()\n",
    "\n",
    "        # Evaluate architectures\n",
    "        network.eval()\n",
    "        rewards = []\n",
    "        old_log_probs_list = []\n",
    "        old_values_list = []\n",
    "        actions_list = []\n",
    "        for _ in range(10):  # Sample 10 architectures\n",
    "            actions, log_probs, values = ppo_controller(len(network.cells))\n",
    "            with torch.no_grad():\n",
    "                acc = evaluate(network, actions, val_data)\n",
    "            rewards.append(acc)\n",
    "            old_log_probs_list.append(log_probs)\n",
    "            old_values_list.append(values)\n",
    "            actions_list.append(actions)\n",
    "\n",
    "        # Update controller using PPO\n",
    "        rewards = torch.tensor(rewards)\n",
    "        old_log_probs = torch.cat(old_log_probs_list)\n",
    "        old_values = torch.cat(old_values_list)\n",
    "        \n",
    "        for _ in range(5):  # PPO update steps\n",
    "            ppo_optim.zero_grad()\n",
    "            loss = ppo_update(ppo_controller, old_log_probs, old_values, rewards, actions_list)\n",
    "            loss.backward()\n",
    "            ppo_optim.step()\n",
    "\n",
    "        print(f\"Epoch {epoch}, Avg Reward: {rewards.mean().item():.4f}\")\n",
    "\n",
    "def evaluate(model, actions, data):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for x, y in data:\n",
    "        with torch.no_grad():\n",
    "            outputs = model(x, actions)\n",
    "            _, predicted = outputs.max(1)\n",
    "            correct += (predicted == y).sum().item()\n",
    "            total += y.size(0)\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have your data loaded as train_data and val_data\n",
    "network = Network(16, 10, 8)  # 16 channels, 10 classes, 8 cells\n",
    "ppo_controller = PPOController(100)\n",
    "# Uncomment the following line to train\n",
    "# train_enas_ppo(network, ppo_controller, train_data, val_data, num_epochs=50)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
